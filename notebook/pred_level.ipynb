{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/hiramatsu/kaggle/hms-harmful-brain-activity-classification/\")\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, LearningRateMonitor,\n",
    "                                         ModelCheckpoint, RichModelSummary,\n",
    "                                         RichProgressBar)\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import wandb\n",
    "from src.conf import TrainConfig\n",
    "from src.datamodule import HMSDataModule\n",
    "from src.modelmodule import HMSModel\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning import LightningModule\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.conf import TrainConfig\n",
    "from src.models.common import get_model\n",
    "from src.utils.augmentation import cutmix_data, mixup_data\n",
    "from src.utils.kaggle_kl_div import score\n",
    "from src.utils.loss_functions import (KLDivLossWithLogits,\n",
    "                                      KLDivLossWithLogitsForVal,\n",
    "                                      KLDWithContrastiveLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMSModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: TrainConfig,\n",
    "        val_df: pd.DataFrame,\n",
    "        fold_id: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model = get_model(cfg)\n",
    "\n",
    "        self.loss_func = KLDWithContrastiveLoss() if (cfg.model.name==\"HMSSpecPararellModel\") or (cfg.model.name==\"HMSSpecEEGPararellModel\")  else KLDivLossWithLogits() \n",
    "\n",
    "        self.validation_step_outputs: list = []\n",
    "        self.best_score = np.inf\n",
    "\n",
    "        self.val_df = val_df\n",
    "        self.fold_id = fold_id            \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor\n",
    "        ):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, t, level_t = batch[\"spec_img\"],  batch[\"target\"], batch['level_target']\n",
    "\n",
    "        if self.cfg.aug.do_mixup and (np.random.random() > 0.5):\n",
    "            img_original, t_original, level_t_original = img[:img.shape[0]//2], t[:t.shape[0]//2], level_t[:level_t.shape[0]//2]\n",
    "            img_mixup, t_mixup, level_t_mixup, index, lam = mixup_data(img[img.shape[0]//2:], t[t.shape[0]//2:], level_t[level_t.shape[0]//2:])\n",
    "            img = torch.cat([img_original, img_mixup], 0)\n",
    "            t = torch.cat([t_original, t_mixup], 0)\n",
    "            level_t = torch.cat([level_t_original, level_t_mixup])\n",
    "            batch[\"spec_img\"] = img\n",
    "            batch[\"target\"] = t\n",
    "            batch['level_target'] = level_t\n",
    "\n",
    "            if self.cfg.use_raw_eeg:\n",
    "                eeg = batch[\"raw_eeg\"]\n",
    "                eeg_original = eeg[:eeg.shape[0]//2]\n",
    "                eeg_mixup = eeg[eeg.shape[0]//2:]\n",
    "                eeg_mixup = lam * eeg_mixup + (1 - lam) * eeg_mixup[index]\n",
    "                eeg = torch.cat([eeg_original, eeg_mixup], 0)\n",
    "                batch[\"raw_eeg\"] = eeg\n",
    "\n",
    "        elif self.cfg.aug.do_cutmix and (np.random.random() > 0.5):\n",
    "            img_original, t_original, level_t_original = img[:img.shape[0]//2], t[:t.shape[0]//2], level_t[:level_t.shape[0]//2]\n",
    "            img_cutmix, t_cutmix, level_t_cutmix, index, lam = cutmix_data(img[img.shape[0]//2:], t[t.shape[0]//2:], level_t[level_t.shape[0]//2:])\n",
    "            img = torch.cat([img_original, img_cutmix], 0)\n",
    "            t = torch.cat([t_original, t_cutmix], 0)\n",
    "            level_t = torch.cat([level_t_original, level_t_cutmix])\n",
    "            batch[\"spec_img\"] = img\n",
    "            batch[\"target\"] = t\n",
    "            batch[\"level_target\"] = level_t\n",
    "\n",
    "            if self.cfg.use_raw_eeg:\n",
    "                eeg = batch[\"raw_eeg\"]\n",
    "                eeg_original = eeg[:eeg.shape[0]//2]\n",
    "                eeg_mixup = eeg[eeg.shape[0]//2:]\n",
    "                eeg_mixup = lam * eeg_mixup + (1 - lam) * eeg_mixup[index]\n",
    "                eeg = torch.cat([eeg_original, eeg_mixup], 0)\n",
    "                batch[\"raw_eeg\"] = eeg\n",
    "\n",
    "        output = self.model(batch)\n",
    "        loss = self.loss_func(output, t, level_t)\n",
    "\n",
    "        if isinstance(loss, dict):\n",
    "            for key in loss.keys():\n",
    "                self.log(\n",
    "                f\"train_{key}\",\n",
    "                loss[key],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                logger=True,\n",
    "                prog_bar=True,\n",
    "                )\n",
    "            \n",
    "            return loss['loss']\n",
    "\n",
    "        else:\n",
    "            self.log(\n",
    "                \"train_loss\",\n",
    "                loss,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                logger=True,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "\n",
    "            return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        t = batch[\"target\"]\n",
    "        level_t = batch['level_target']\n",
    "        self.model.training = False\n",
    "        output = self.model(batch)\n",
    "        loss = self.loss_func(output, t, level_t)\n",
    "\n",
    "        if isinstance(output, dict):\n",
    "            output = output['weighted_output']\n",
    "\n",
    "\n",
    "        if isinstance(loss, dict):\n",
    "            for key in loss.keys():\n",
    "                self.log(\n",
    "                f\"valid_{key}\",\n",
    "                loss[key],\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                logger=True,\n",
    "                prog_bar=True,\n",
    "                )\n",
    "            \n",
    "            loss = loss['loss']\n",
    "\n",
    "        else:\n",
    "            self.log(\n",
    "                \"valid_loss\",\n",
    "                loss,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                logger=True,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "\n",
    "        self.validation_step_outputs.append(\n",
    "            (   \n",
    "                t.detach().cpu().numpy(),\n",
    "                output.softmax(dim=1).detach().cpu(),\n",
    "                loss.detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        labels = np.concatenate([x[0] for x in self.validation_step_outputs])\n",
    "        preds = np.concatenate([x[1] for x in self.validation_step_outputs])\n",
    "        losses = np.array([x[2] for x in self.validation_step_outputs])\n",
    "        loss = losses.mean()\n",
    "\n",
    "        val_pred_df = pd.DataFrame(preds, columns=self.cfg.labels)\n",
    "\n",
    "        val_pred_df.insert(0, \"label_id\", self.val_df[\"label_id\"].values)\n",
    "\n",
    "        val_score = score(solution=self.val_df[[\"label_id\"] + self.cfg.labels].copy().reset_index(drop=True), \n",
    "                          submission=val_pred_df, \n",
    "                          row_id_column_name='label_id')\n",
    "\n",
    "        self.log(\"valid_score\", val_score, on_step=False, on_epoch=True, logger=True, prog_bar=True)\n",
    "\n",
    "        if val_score < self.best_score:\n",
    "            np.save(self.cfg.dir.save_dir + f\"/fold_{self.fold_id}/labels.npy\", labels)\n",
    "            np.save(self.cfg.dir.save_dir + f\"/fold_{self.fold_id}/preds.npy\", preds)\n",
    "            val_pred_df.insert(0, \"label_id\", self.val_df[\"label_id\"].values)\n",
    "            val_pred_df.to_csv(self.cfg.dir.save_dir + f\"/fold_{self.fold_id}/val_pred_df.csv\", index=False)\n",
    "            torch.save(self.model.state_dict(), self.cfg.dir.save_dir + f\"/fold_{self.fold_id}/best_model.pth\")\n",
    "            print(f\"Saved best model {self.best_score} -> {val_score}\")\n",
    "            self.best_score = val_score\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.cfg.optimizer.lr)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_training_steps=self.trainer.max_steps, **self.cfg.scheduler\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"/home/hiramatsu/kaggle/hms-harmful-brain-activity-classification/result/exp045/.hydra/config.yaml\")\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "save_dir = cfg.dir.save_dir\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_scores = []\n",
    "folds = []\n",
    "\n",
    "for fold_id in range(cfg.n_folds):\n",
    "    os.makedirs(save_dir + f\"/fold_{fold_id}\", exist_ok=True)\n",
    "    # init lightning model\n",
    "    datamodule = HMSDataModule(cfg, fold_id)\n",
    "\n",
    "    model = HMSModel(\n",
    "        cfg, datamodule.val_df, fold_id\n",
    "    )\n",
    "\n",
    "    # set callbacks\n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        dirpath=save_dir+f\"/fold_{fold_id}\",\n",
    "        verbose=True,\n",
    "        monitor=cfg.trainer.monitor,\n",
    "        mode=cfg.trainer.monitor_mode,\n",
    "        save_top_k=1,\n",
    "        save_last=False,\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(\"epoch\")\n",
    "    progress_bar = RichProgressBar()\n",
    "    early_stopping = EarlyStopping(monitor='valid_score', patience=cfg.early_stopping_rounds)\n",
    "    model_summary = RichModelSummary(max_depth=2)\n",
    "\n",
    "    trainer = Trainer( \n",
    "        # env\n",
    "        default_root_dir=Path.cwd(),\n",
    "        # num_nodes=cfg.training.num_gpus,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        devices=cfg.trainer.device,\n",
    "        precision=16 if cfg.trainer.use_amp else 32,\n",
    "        # training\n",
    "        fast_dev_run=cfg.trainer.debug,  # run only 1 train batch and 1 val batch\n",
    "        max_epochs=cfg.trainer.epochs,\n",
    "        max_steps=cfg.trainer.epochs * len(datamodule.train_dataloader()),\n",
    "        gradient_clip_val=cfg.trainer.gradient_clip_val,\n",
    "        accumulate_grad_batches=cfg.trainer.accumulate_grad_batches,\n",
    "        callbacks=[checkpoint_cb, lr_monitor, progress_bar, model_summary, early_stopping],\n",
    "        # resume_from_checkpoint=resume_from,\n",
    "        num_sanity_val_steps=0,\n",
    "        log_every_n_steps=int(len(datamodule.train_dataloader()) * 0.1),\n",
    "        sync_batchnorm=True,\n",
    "        check_val_every_n_epoch=cfg.trainer.check_val_every_n_epoch,\n",
    "        \n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    best_scores.append(model.best_score)\n",
    "    folds.append(f\"fold_{fold_id}\")\n",
    "    print(f'fold_{fold_id}: best_score is {model.best_score}')\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "best_scores.append(np.mean(best_scores))\n",
    "folds.append(f\"mean\")\n",
    "\n",
    "print(f'CV score is {np.mean(best_scores)}.')\n",
    "\n",
    "best_score_df = pd.DataFrame(data = {\"fold_id\": folds, \"scores\": best_scores})\n",
    "\n",
    "best_score_df.to_csv(save_dir + '/best_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
